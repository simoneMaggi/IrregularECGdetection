{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f37d9f2",
   "metadata": {},
   "source": [
    "# AAECG\n",
    "\n",
    "Train procedure of the AAECG - Adversarial Auto Encoder adapted to recognize irregular heartbeats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294fc937",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2970d47a",
   "metadata": {},
   "source": [
    "## Some Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "def get_beat_info(beat_path):\n",
    "    with open(beat_path+\".pkl\", \"rb\") as pkl_handle:\n",
    "        res = pickle.load(pkl_handle)\n",
    "    del res['medications']\n",
    "    return res\n",
    "    \n",
    "def plot_some_beat(beat_list, titles_list = None, suptitle = None, savefig = False, file= None):\n",
    "    n = len(beat_list)\n",
    "    nl = len(beat_list[0])\n",
    "    f, ax = plt.subplots(nl, n, sharey=True)\n",
    "    if not(suptitle is None):\n",
    "        f.suptitle(suptitle)\n",
    "    for i in range(n):\n",
    "        if not (titles_list is None):\n",
    "            if nl > 1:\n",
    "                ax[0,i].set_title(titles_list[i])\n",
    "            else:\n",
    "                ax[i].set_title(titles_list[i])\n",
    "        if nl == 1:\n",
    "            ax[i].plot(beat_list[i][0])\n",
    "            ax[i].set_ylim(bottom = -1, top = 1)\n",
    "        else:\n",
    "            for j in range(nl):\n",
    "                ax[j,i].plot(beat_list[i][j])\n",
    "                ax[j,i].set_ylim(bottom = -1, top = 1)\n",
    "    if savefig:\n",
    "        plt.savefig(file, dpi = 300)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f249d",
   "metadata": {},
   "source": [
    "# ECG dataset class which implements the Pytorch Dataset interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ecgdataset(Dataset):\n",
    "    def __init__(self, folder_path, db = None, upload_on_ram = False, batch_size = 500, shuffle = True):\n",
    "        if not (db is None):\n",
    "            self.upload_on_ram = True\n",
    "            self.db = db\n",
    "            if shuffle:\n",
    "                random.shuffle(self.db)\n",
    "            self.data_len = len(db)\n",
    "            return\n",
    "        \n",
    "        self.folder_path = folder_path\n",
    "        # Get ecg list\n",
    "        self.beat_list = glob.glob(folder_path + '*.npy')\n",
    "        \n",
    "        # Calculate len\n",
    "        self.data_len = len(self.beat_list)\n",
    "        \n",
    "        # Upload on Ram to speed up (use only if your hardware has enough memory)\n",
    "        self.upload_on_ram = upload_on_ram\n",
    "        \n",
    "        if self.upload_on_ram:\n",
    "            print(\"\\n uploading \"+folder_path+\" on RAM\")\n",
    "            self.db = Parallel(n_jobs=-1, verbose = 1, backend=\"multiprocessing\", batch_size= batch_size)(delayed(self.load_index)(i)\n",
    "                           for i in range(self.data_len)) \n",
    "            if shuffle:\n",
    "                random.shuffle(self.db)\n",
    "    def get_db(self):\n",
    "        return self.db\n",
    "    \n",
    "    def encode_sex(self, sex):\n",
    "        res = [0, 0, 1]\n",
    "        if sex == 'F':\n",
    "            res = [1, 0, 0]\n",
    "        elif sex == 'M':\n",
    "            res = [0, 1, 0]\n",
    "        return torch.tensor(res, dtype = torch.float32)\n",
    "    \n",
    "    def load_index(self, index):\n",
    "        beat_path = self.beat_list[index]\n",
    "        \n",
    "        # Get ecg data\n",
    "        beat = torch.from_numpy(np.load(beat_path)).float()\n",
    "        \n",
    "        # Get ecg labels. The file name is of the type Rxxx_xxx.npy\n",
    "        info = get_beat_info(beat_path.split('.')[0])\n",
    "        \n",
    "        sex = info['sex']\n",
    "        sex = self.encode_sex(sex)\n",
    "        label = sex\n",
    "        \n",
    "        return (beat, label, info)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.upload_on_ram:\n",
    "            return self.db[index]\n",
    "        else:\n",
    "            return self.load_index(index)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd647a1",
   "metadata": {},
   "source": [
    "# AAECG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72751c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, precision_recall_curve, fbeta_score\n",
    "from tqdm import tqdm\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class CBeatAAE:\n",
    "    #ENCODER\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, ngpu, L, N, nef, nz):\n",
    "            super(CBeatAAE.Encoder, self).__init__()\n",
    "            self.ngpu = ngpu\n",
    "            self.nz = nz\n",
    "            self.conv = nn.Sequential(\n",
    "                # input Beat dimensions L x N => L x N \n",
    "                nn.Conv1d( L, nef, 4, 2, 1, bias=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                # state size (nef) x 128 \n",
    "                nn.Conv1d( nef, nef * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm1d(nef*2),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                # state size. (nef*2) x 64\n",
    "                nn.Conv1d(nef * 2, nef * 4, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm1d(nef*4),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                # state size. (nef*4) x 32\n",
    "                nn.Conv1d( nef * 4, nef * 8, 3, 2, 0, bias=False),\n",
    "                nn.BatchNorm1d(nef*8),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                # state size. (nef*8) x 16\n",
    "                nn.Conv1d( nef * 8, nef * 16, 3, 2, 0, bias=False),\n",
    "                nn.BatchNorm1d(nef*16),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "                # state size. (nef * 16) x 8\n",
    "                )\n",
    "            self.mu = nn.Sequential(\n",
    "                nn.Conv1d( nef * 16, nz, 8, 1, 0),\n",
    "                # state size. (nef * 32) x 1\n",
    "                nn.Flatten()\n",
    "                )\n",
    "            self.logvar = nn.Sequential(\n",
    "                nn.Conv1d( nef * 16, nz, 8, 1, 0),\n",
    "                # state size. (nef * 32) x 1\n",
    "                nn.Flatten()\n",
    "                )\n",
    "            \n",
    "        \n",
    "        def reparametrization(self, mu, logvar):\n",
    "            std = torch.exp(logvar/2)\n",
    "            sampled_z = torch.randn((mu.size(0), self.nz), device = mu.device)\n",
    "            z = sampled_z * std + mu\n",
    "            \n",
    "            return z\n",
    "        \n",
    "        def features(self, x):\n",
    "            return self.conv(x)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            f = self.conv(x)\n",
    "            return self.reparametrization(self.mu(f), self.logvar(f))\n",
    "        \n",
    "        def variance(self, x):\n",
    "            f = self.conv(x)\n",
    "            std = torch.exp(self.logvar(f)/2)\n",
    "            return std\n",
    "    \n",
    "    #DECODER\n",
    "    \n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, ngpu, nef, nz, nc, L):\n",
    "            super(CBeatAAE.Decoder, self).__init__()\n",
    "            self.ngpu = ngpu\n",
    "            self.upconv = nn.Sequential(\n",
    "                # state size (nef*16) x 8 \n",
    "                nn.ConvTranspose1d( nz + nc, nef * 16, 8, 1, 0, bias=False),\n",
    "                nn.BatchNorm1d(nef * 16),\n",
    "                nn.ReLU(True),\n",
    "                # state size. (nef*8) x 16 \n",
    "                nn.ConvTranspose1d(nef * 16, nef * 8, 3, 2, 0, bias=False),\n",
    "                nn.BatchNorm1d(nef * 8),\n",
    "                nn.ReLU(True),\n",
    "                # state size. (nef*4) x 32 \n",
    "                nn.ConvTranspose1d( nef * 8, nef * 4, 3, 2, 0, bias=False),\n",
    "                nn.BatchNorm1d(nef * 4),\n",
    "                nn.ReLU(True),\n",
    "                # state size. (nef*2) x 64 \n",
    "                nn.ConvTranspose1d( nef * 4, nef * 2, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm1d(nef*2),\n",
    "                nn.ReLU(True),\n",
    "                # state size. \n",
    "                nn.ConvTranspose1d( nef * 2, nef, 4, 2, 1, bias=False),\n",
    "                nn.BatchNorm1d(nef),\n",
    "                nn.ReLU(True),\n",
    "                # state size. (nef) x 128 \n",
    "                nn.ConvTranspose1d( nef, L, 4, 2, 1, bias=False),\n",
    "                # state size. (L) x 256 \n",
    "                nn.Tanh()\n",
    "            )\n",
    "    \n",
    "        def forward(self, z, labels):\n",
    "            l = torch.cat((z,labels), dim = 1)\n",
    "            return self.upconv(l.unsqueeze(2))\n",
    "    \n",
    "    \n",
    "    # DISCRIMINATOR\n",
    "    \n",
    "    class Discriminator_prior(nn.Module):\n",
    "        def __init__(self, ngpu, nz, ndf):\n",
    "            super(CBeatAAE.Discriminator_prior, self).__init__()\n",
    "            self.ngpu = ngpu\n",
    "            self.main = nn.Sequential(\n",
    "                nn.Linear(in_features=(nz), out_features=ndf),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                \n",
    "                nn.Linear(in_features=ndf, out_features=ndf//2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                \n",
    "                nn.Linear(in_features=ndf//2, out_features=ndf//4),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                \n",
    "                nn.Linear(in_features=ndf//4, out_features=1)\n",
    "            )\n",
    "    \n",
    "        def forward(self, input):\n",
    "            return self.main(input)\n",
    "    \n",
    "    \n",
    "    def __init__(self, device = torch.device('cpu'), N=280, L=1, nz=3,nc=3,nef=32,ndf=32, ngpu = 0):\n",
    "        # Lenght of a beat.\n",
    "        self.N = N\n",
    "        \n",
    "        # Number of leads\n",
    "        self.L = L\n",
    "        \n",
    "        # Size of z latent vector \n",
    "        self.nz = nz\n",
    "        \n",
    "        # Size of additional information vector, one hot encoded sex value\n",
    "        self.nc = nc # Example: [0, 1, 0] => [male, female, unlabeled] \n",
    "        \n",
    "        # Size of feature maps in encoder\n",
    "        self.nef = nef\n",
    "        \n",
    "        # Size of feature maps in discriminator\n",
    "        self.ndf = ndf\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.netE = self.Encoder(ngpu, L, N,nef, nz) \n",
    "        self.netD = self.Decoder(ngpu, nef, nz, nc, L)\n",
    "        self.netDis_prior = self.Discriminator_prior(ngpu, nz, ndf)\n",
    "        \n",
    "        self.netE.to(device)\n",
    "        self.netD.to(device)\n",
    "        self.netDis_prior.to(device)\n",
    "        \n",
    "        # Initialize\n",
    "        self.initialize()\n",
    "        \n",
    "        self.trained = False\n",
    "        \n",
    "        # Initialize the threeshold\n",
    "        self.threeshold = 0\n",
    "        \n",
    "        # Covariance\n",
    "        self.invSigma = np.diag(np.ones(self.N*self.L))\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.netE.apply(self.weights_init)\n",
    "        self.netD.apply(self.weights_init)\n",
    "        self.netDis_prior.apply(self.weights_init)\n",
    "        \n",
    "    def sample_noise(self, b_s, uniform_noise = False):\n",
    "        if uniform_noise:\n",
    "            # Uniform Distribution [-1, 1]\n",
    "            a = -1\n",
    "            b = 1\n",
    "            noise = (a - b) * torch.rand((b_s, self.nz), device=self.device, dtype=torch.float) + b\n",
    "        else:\n",
    "            noise = torch.randn((b_s, self.nz), device = self.device, dtype = torch.float)\n",
    "        return noise\n",
    "    \n",
    "    def weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "        elif classname.find('Linear') != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    \n",
    "    def plot_error_mixing(self, errors, labels, best_thr, est_thr = None,):\n",
    "        plt.figure()\n",
    "        h = np.histogram(errors[np.where(labels == 0)])\n",
    "        plt.plot(h[1][:-1], h[0]/np.sum(h[0]), label = 'normal')\n",
    "        h = np.histogram(errors[np.where(labels == 1)])\n",
    "        plt.plot( h[1][:-1], h[0]/np.sum(h[0]), label = 'abnormal')\n",
    "        plt.ylim([0, 1.001])\n",
    "        plt.xlim([0, np.max(errors)])\n",
    "        if est_thr is None:\n",
    "            plt.vlines(x = best_thr, ymin = 0, ymax = 1, colors = 'b', label = 'threeshold')\n",
    "        else:\n",
    "            plt.vlines(x = est_thr, ymin = 0, ymax = 1, colors = 'b', label = 'alpha-thr')\n",
    "            plt.vlines(x = best_thr, ymin = 0, ymax = 1, colors = 'r', label = 'best f1-thr')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_threeshold(self, errors, labels = None, alpha = 0.05, verbose = False):\n",
    "        # Estimate threeshold only with normal data\n",
    "        # fixing false positive rate to alpha.\n",
    "        # Then, if provided, use the labels to compute the threeshold\n",
    "        # which gives the best recall ( = accuracy)\n",
    "        # if the threeshold is greater than the previously computed\n",
    "        # that means that we have decreased also the false positive rate\n",
    "        # Hence we select the new threeshold\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"estimate threeshold with only normal data\")\n",
    "        if labels is None:\n",
    "            ecdf = ECDF(errors)\n",
    "        else:\n",
    "            ecdf = ECDF(errors[np.where(labels == 0)])\n",
    "        thr = ecdf.x[np.where(ecdf.y >= (1-alpha))[0][0]]\n",
    "        if labels is None:\n",
    "            return thr, _\n",
    "        quantile_thr = thr\n",
    "        best_thr = 0\n",
    "        # Estimate threeshold with both\n",
    "        thrs = np.linspace(np.min(errors), np.max(errors), num=errors.shape[0]*2)\n",
    "        if verbose:\n",
    "            print(\"estimate threeshold with all data\")\n",
    "        e = Parallel(n_jobs=num_cores)(delayed(fbeta_score)(errors >= t, labels, beta = 2)\n",
    "                               for t in thrs)\n",
    "        ind = np.argmax(np.array(e))\n",
    "        best_thr = thrs[ind]\n",
    "        if verbose:\n",
    "           self.plot_error_mixing(errors, labels, best_thr, est_thr = thr)\n",
    "        \n",
    "        return quantile_thr, best_thr\n",
    "    \n",
    "    \n",
    "    def get_anomaly_score(self, X, label, L=1):\n",
    "        # X is b_s x L x N\n",
    "        b_s = X.size(0) # Batch size\n",
    "        AS = 0\n",
    "        X_det = X.view(b_s,-1).detach().cpu().numpy()\n",
    "        for l in range(L):\n",
    "            X_rec = self.netD(self.netE(X), label).view(b_s,-1).detach().cpu().numpy()\n",
    "            AS += np.mean(np.square(X_rec - X_det), axis = 1)\n",
    "        return AS/L\n",
    "    \n",
    "    def get_rec_errors(self, X, label):\n",
    "        b_s = X.size(0) # Batch size\n",
    "        X_rec = self.netD(self.netE(X), label).view(b_s,-1).detach().cpu().numpy()\n",
    "        X = X.view(b_s,-1).detach().cpu().numpy() # (b_s, NxL)\n",
    "        rec_errors = np.square(X_rec-X)\n",
    "        return rec_errors\n",
    "\n",
    "    \n",
    "    def compute_gradient_penalty(self, real_samples, fake_samples, labels = None):\n",
    "        \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "        # Random weight term for interpolation between real and fake samples\n",
    "        if labels is None:\n",
    "            alpha = torch.rand((real_samples.size(0), 1), device = self.device)\n",
    "        else:\n",
    "            alpha = torch.rand((real_samples.size(0), 1, 1), device = self.device)\n",
    "        # Get random interpolation between real and fake samples\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        if labels is None:\n",
    "            d_interpolates = self.netDis_prior(interpolates)\n",
    "        fake = torch.ones((real_samples.size(0), 1), device = self.device)\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "    \n",
    "    \n",
    "    def train(self, train_dl, valid_dl, num_epochs, lr = 0.0001, beta1=0.5, beta2 = 0.9, lambda_rec = 1,\n",
    "              lambda_adv = 1, lambda_gp = 10, lambda_tv = 0.0001,\n",
    "               n_critic = 5, model_folder = 'models/AAE/'):                                                                             \n",
    "        \n",
    "        beat_list = []\n",
    "        E_losses = []\n",
    "        MSE_losses = []\n",
    "        ADV_losses = []\n",
    "        Disc_prior_losses = []\n",
    "        Disc_visible_losses = []\n",
    "        TV_losses = []\n",
    "        best_metric = 0\n",
    "        if not os.path.exists(model_folder):\n",
    "            os.makedirs(model_folder)\n",
    "            \n",
    "        fixed_labels = torch.tensor([[1,0,0],\n",
    "                      [1,0,0],\n",
    "                      [1,0,0],\n",
    "                      [0,1,0],\n",
    "                      [0,1,0],\n",
    "                      [0,1,0]]).to(self.device)\n",
    "        fixed_noise = self.sample_noise(len(fixed_labels))\n",
    "        \n",
    "        ## Initialize Loss functions\n",
    "        MSE = nn.MSELoss().to(self.device)\n",
    "        \n",
    "        # Setup Adam optimizers for both Enc, Dec and Disc \n",
    "        optimizerE = optim.Adam(self.netE.parameters(), lr=lr, betas = (beta1, beta2))\n",
    "        optimizerD = optim.Adam(self.netD.parameters(), lr=lr, betas = (beta1, beta2))\n",
    "        optimizerDis_prior = optim.Adam(self.netDis_prior.parameters(), lr=lr, betas = (beta1, beta2))\n",
    "        schedulers = [ReduceLROnPlateau(optimizerE, patience = 10),\n",
    "                     ReduceLROnPlateau(optimizerD, patience = 10),\n",
    "                     ReduceLROnPlateau(optimizerDis_prior, patience = 10)]\n",
    "                    \n",
    "        \n",
    "        print(\"Starting Training Loop...\")\n",
    "        # For each epoch\n",
    "        for epoch in range(num_epochs):\n",
    "            try:\n",
    "                # For each batch in the dataloader\n",
    "                for i, data in enumerate(train_dl, 0):\n",
    "                    # Format batch\n",
    "                    X = data[0].to(self.device)\n",
    "                    label = data[1].to(self.device)\n",
    "                    b_size = X.size(0)\n",
    "                    ############################\n",
    "                    # (2) Update Discriminator network on latent: minimize wasserstein distance\n",
    "                    ###########################\n",
    "                    for _ in range(n_critic):\n",
    "                        self.netDis_prior.zero_grad()\n",
    "                        data_n = next(iter(train_dl))\n",
    "                        X_n = data_n[0].to(self.device)\n",
    "                        Z_prior = self.sample_noise(b_size) \n",
    "                        Z_latent = self.netE(X_n).detach()\n",
    "                        Disc_z_prior = self.netDis_prior(Z_prior).view(-1)\n",
    "                        Disc_z_real = torch.mean(Disc_z_prior)\n",
    "                        Disc_z = self.netDis_prior(Z_latent).view(-1)\n",
    "                        Disc_z_fake = torch.mean(Disc_z)\n",
    "                        gp = self.compute_gradient_penalty(Z_prior, Z_latent)\n",
    "                        W_distance = Disc_z_real - Disc_z_fake \n",
    "                        loss = -1*W_distance + lambda_gp * gp\n",
    "                        # Calculate gradients for Disc in backward pass\n",
    "                        loss.backward()\n",
    "                        optimizerDis_prior.step()\n",
    "                    \n",
    "                    ##############################\n",
    "                    # (1) Update D and E networks:\n",
    "                    ##############################\n",
    "                    self.netE.zero_grad()\n",
    "                    self.netD.zero_grad()\n",
    "                    \n",
    "                    Z = self.netE(X)\n",
    "                    X_rec = self.netD(Z, label)\n",
    "                    X_rec_detached = self.netD(Z.detach(), label)\n",
    "                    \n",
    "                    # total variation reg\n",
    "                    TV_loss = torch.div(torch.abs(X_rec_detached[:,:,1:] - X_rec_detached[:,:,:-1]).sum(), b_size)\n",
    "                    \n",
    "                    # rec losses\n",
    "                    ED_mse =  MSE(X, X_rec) \n",
    "\n",
    "                    # adv latent loss\n",
    "                    E_advz =  torch.mean(self.netDis_prior(Z).view(-1))\n",
    "                    \n",
    "                    total_loss = lambda_rec * ED_mse + lambda_tv * TV_loss -1* lambda_adv *E_advz\n",
    "                    total_loss.backward()\n",
    "                    optimizerE.step()\n",
    "                    optimizerD.step()\n",
    "                    \n",
    "                    ## Output training stats\n",
    "                    if i % 50 == 0:\n",
    "                        print('[%d/%d][%d/%d]\\n\\t mse Loss: %.4f \\n\\t  Adv loss Encoder: %.4f \\n\\t Loss Disc prior: %.4f  fake: %.4f real: %.4f \\n\\t  TV loss %.4f'\n",
    "                              % (epoch, num_epochs, i, len(train_dl),\n",
    "                                 ED_mse.item(), E_advz.item(), W_distance.item(),Disc_z_fake.item(), Disc_z_real.item(), TV_loss.item()))\n",
    "            \n",
    "                    # Save Losses for plotting later\n",
    "                    E_losses.append(E_advz.item())\n",
    "                    MSE_losses.append(ED_mse.item())\n",
    "                    Disc_prior_losses.append(W_distance.item())\n",
    "                    TV_losses.append(TV_loss.item())\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n INTERRUPT DETECTED: training stopped\")\n",
    "                break\n",
    "            # evaluate and save best model\n",
    "            print(\"\\n Evaluate\")\n",
    "            self.compute_threeshold(valid_dl, optimal=True)\n",
    "            labels, pred = self.evaluate(valid_dl)\n",
    "            metric = average_precision_score(labels, pred)\n",
    "            for s in schedulers:\n",
    "                s.step(-1*metric)\n",
    "            if metric >= best_metric:\n",
    "                print(\"\\n %.4f -----> %.4f \"%(best_metric, metric))\n",
    "                best_metric = metric\n",
    "                self.save(model_folder)\n",
    "            else:\n",
    "                print(\"\\n %.4f less than best %.4f\"%(metric, best_metric))\n",
    "            # Check how the generator is doing by saving G's output on fixed_noise\n",
    "            with torch.no_grad():\n",
    "                fake = self.netD(fixed_noise, fixed_labels).detach().cpu()\n",
    "                plot_some_beat(fake, savefig=True, file = model_folder+\"/epoch_\"+str(epoch)+\".png\")\n",
    "                if epoch % 10 == 0:\n",
    "                    beat_list.append(fake)\n",
    "            \n",
    "        \n",
    "        self.load_model(model_folder)\n",
    "        \n",
    "        f, ax = plt.subplots(3, 1, sharex = True)\n",
    "        ax[0].set_title(\"Adversarial Losses \")\n",
    "        ax[0].plot(MSE_losses, label='mse error')\n",
    "        ax[0].plot(ADV_losses, label = \"adv loss\")\n",
    "        ax[0].plot(E_losses, label = 'Adv aggregate error')\n",
    "        ax[0].legend()\n",
    "        ax[1].set_title(\"Discriminator losses\")\n",
    "        ax[1].plot(Disc_prior_losses, label = 'latent')\n",
    "        ax[1].plot(Disc_visible_losses, label = 'visible')\n",
    "        ax[1].legend()\n",
    "        ax[1].set_ylabel(\"Loss\")\n",
    "        ax[2].plot(TV_losses, label= \"total variation\")\n",
    "        ax[2].legend()\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.show()\n",
    "        for i in range(len(beat_list)):\n",
    "            plot_some_beat(beat_list[i], suptitle = 'epoch: '+str(i))\n",
    "        self.trained = True\n",
    "        \n",
    "    def save(self, folder = '/'):\n",
    "        torch.save(self.netDis_prior.state_dict(), folder+\"discriminator_prior.mod\")\n",
    "        torch.save(self.netE.state_dict(), folder+\"encoder.mod\")\n",
    "        torch.save(self.netD.state_dict(), folder+\"decoder.mod\")\n",
    "        np.save(folder+\"thr.npy\", np.array([self.threeshold]))\n",
    "        \n",
    "        \n",
    "    def compute_threeshold(self, dl, optimal = False, verbose = False):\n",
    "        pred = None\n",
    "        labels = None\n",
    "        if optimal:\n",
    "            print(\"\\n\\t Computing the optimal threeshold\")\n",
    "        else:\n",
    "            print(\"\\n\\t Computing thr\")\n",
    "        if verbose:       \n",
    "            pbar = tqdm(total = len(valid_dl))\n",
    "        for i,data in enumerate(valid_dl, 0):\n",
    "            beats = data[0].to(self.device)\n",
    "            codes = data[1].to(self.device)\n",
    "            label = np.array(data[2]['label']) == 'abnormal' # 0 normale 1 anormale\n",
    "            ano_sc = self.get_anomaly_score(beats, codes)\n",
    "            if pred is None:\n",
    "                pred = ano_sc\n",
    "                labels = label\n",
    "            else:\n",
    "                pred = np.concatenate((pred, ano_sc), axis = 0)\n",
    "                labels = np.concatenate((labels, label), axis = 0)\n",
    "            if verbose:\n",
    "                pbar.update(n = 1)\n",
    "        if verbose:\n",
    "            pbar.close()\n",
    "        labels = np.array(labels)\n",
    "        # Compute threeshold using both errors\n",
    "        if optimal:\n",
    "            quantile_thr, best_thr = self.get_threeshold(pred, labels = labels, verbose = verbose)\n",
    "            self.threeshold = best_thr\n",
    "        else:\n",
    "            quantile_thr, _ = self.get_threeshold(pred, labels = None, verbose = verbose)\n",
    "            self.threeshold = quantile_thr\n",
    "            \n",
    "        \n",
    "    def evaluate(self, dl, verbose = False):\n",
    "        pred = None\n",
    "        labels = None\n",
    "        if verbose:\n",
    "            pbar = tqdm(total=len(dl))\n",
    "        for i,data in enumerate(test_dl, 0):\n",
    "            beats = data[0].to(self.device)\n",
    "            codes = data[1].to(self.device)\n",
    "            label = np.array(data[2]['label']) == 'abnormal' # 0 normale 1 anormale\n",
    "            ano_sc = self.get_anomaly_score(beats, codes)\n",
    "            if pred is None:\n",
    "                pred = ano_sc\n",
    "                labels = label\n",
    "            else:\n",
    "                pred = np.concatenate((pred, ano_sc), axis = 0)\n",
    "                labels = np.concatenate((labels, label), axis = 0)\n",
    "            if verbose:\n",
    "                pbar.update(n=1)\n",
    "        if verbose:\n",
    "            pbar.close()\n",
    "        return labels, pred\n",
    "        \n",
    "    def test(self, test_dl, result_folder = 'models/AAE/'):\n",
    "        if not self.trained:\n",
    "            print(\"train or load a model\")\n",
    "            return None\n",
    "        print(\"\\n Starting Testing Loop... \\n\")\n",
    "        labels,pred = self.evaluate(test_dl)\n",
    "        \n",
    "        clrp = classification_report(labels, pred >= self.threeshold,\n",
    "                                         target_names=['normal', 'abnormal'])\n",
    "        print(\"\\n\")\n",
    "        print(clrp)\n",
    "        self.plot_error_mixing(pred, labels, self.threeshold)\n",
    "        clrp = classification_report(labels, pred >= self.threeshold,\n",
    "                                         target_names=['normal', 'abnormal'], output_dict = True)\n",
    "        auc = roc_auc_score(labels, pred)\n",
    "        pr_auc = average_precision_score(labels, pred)\n",
    "        result = {'rep':clrp['abnormal'], 'roc_auc':auc, 'pr_auc':pr_auc,\n",
    "                  'pr_curve':precision_recall_curve( labels, pred), \n",
    "                  'f2score': fbeta_score(labels, pred >= self.threeshold, beta = 2)}\n",
    "        \n",
    "        with open(result_folder+'result.pickle', 'wb') as handle:\n",
    "            pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        return classification_report(labels, pred >= self.threeshold,\n",
    "                                         target_names=['normal', 'abnormal'],\n",
    "                                         output_dict=True)\n",
    "    def load_model(self, model_folder='/'):\n",
    "        self.netE.load_state_dict(torch.load(model_folder+\"encoder.mod\", map_location=self.device))\n",
    "        self.netD.load_state_dict(torch.load(model_folder+\"decoder.mod\", map_location=self.device))\n",
    "        self.netDis_prior.load_state_dict(torch.load(model_folder+\"discriminator_prior.mod\", map_location = self.device))\n",
    "        # Load the threeshold\n",
    "        self.threeshold = np.load(model_folder+\"thr.npy\")\n",
    "        self.trained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627d23a",
   "metadata": {},
   "source": [
    "## Inizialize the Training and Testing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "from shutil import copyfile,rmtree\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Root directory \n",
    "root = \"/storage/intra/\"\n",
    "path_normal = root+'normal/'\n",
    "path_abnormal = root+'abnormal/'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "# Datasets\n",
    "normal_ecg = Ecgdataset(path_normal, upload_on_ram = True)\n",
    "normal_ecg = normal_ecg.get_db()\n",
    "\n",
    "abnormal_ecg = Ecgdataset(path_abnormal, upload_on_ram = True)\n",
    "abnormal_ecg = abnormal_ecg.get_db()\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 256\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Decide which device we want to run on\n",
    "if ngpu >0:\n",
    "    device = torch.device(\"cuda:0\" if ( ngpu > 0 and torch.cuda.is_available()) else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cbdf2e",
   "metadata": {},
   "source": [
    "## 5-fold cross validation. The results for each for are automatically saved in a specified folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d286bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBeatAAE(device= device, nz = 5)\n",
    "model_name = 'AAE'\n",
    "\n",
    "# If the first i fold has already been done\n",
    "start = 0\n",
    "n_fold = 5\n",
    "normal_valid_perc = 0.05\n",
    "abnormal_valid_perc = 0.1\n",
    "\n",
    "for i in range(start, n_fold):\n",
    "    train_ecg = normal_ecg.copy()\n",
    "    \n",
    "    # Test ECG\n",
    "    test_ecg = abnormal_ecg.copy()\n",
    "    # select the normal to put in the test\n",
    "    test_ecg.extend(train_ecg[int(i/n_fold*len(normal_ecg)): int((i+1)/n_fold*len(normal_ecg))])\n",
    "    train_ecg[int(i/n_fold*len(normal_ecg)): int((i+1)/n_fold*len(normal_ecg))] = []\n",
    "    \n",
    "    # Validation ECG \n",
    "    valid_ecg = train_ecg[:int(normal_valid_perc*len(train_ecg))]\n",
    "    train_ecg[:int(normal_valid_perc*len(train_ecg))] = []\n",
    "    \n",
    "    valid_ecg.extend(test_ecg[:int(abnormal_valid_perc*len(abnormal_ecg))])\n",
    "    test_ecg[:int(abnormal_valid_perc*len(abnormal_ecg))] = []\n",
    "    \n",
    "    # Create the datasets\n",
    "    train_set = Ecgdataset(None, db = train_ecg) \n",
    "    validation_set = Ecgdataset(None, db = valid_ecg)\n",
    "    test_set = Ecgdataset(None, db = test_ecg)\n",
    "\n",
    "    # Create the dataloader\n",
    "    train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True, drop_last=True)\n",
    "    valid_dl = torch.utils.data.DataLoader(validation_set, batch_size=batch_size*10)\n",
    "    test_dl = torch.utils.data.DataLoader(test_set, batch_size = batch_size*10)\n",
    "    \n",
    "    model.initialize()\n",
    "    model.train(train_dl, valid_dl, num_epochs, lambda_rec = 1, lambda_adv = 1,lambda_tv = 0.001,beta1 = 0, model_folder = '/storage/models/')\n",
    "    \n",
    "    if not os.path.exists(\"/storage/models/\"+model_name+\"/fold_\"+str(i)+\"/\"):\n",
    "        os.makedirs(\"/storage/models/\"+model_name+\"/fold_\"+str(i)+\"/\")\n",
    "    \n",
    "    model.test(test_dl, result_folder=\"/storage/models/\"+model_name+\"/fold_\"+str(i)+\"/\")\n",
    "    \n",
    "    del train_ecg\n",
    "    del valid_ecg\n",
    "    del test_ecg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
